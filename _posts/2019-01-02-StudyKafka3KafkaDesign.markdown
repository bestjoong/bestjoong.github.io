---
layout: post
title:  "[Study][Kafka] 카프카 디자인
date:   2018-01-02 09:20:10 +0900
author: Hyunjoong Kim
categories: Study
tags: Kafka
---



* 3.1 카프카 디자인의 특징
  * 3.1.1 분산 시스템
    * 단일 시스템보다 더 높은 성능을 얻을 수 있다.
        * 분산 시스템 중 하나의 서버 또는 노드 등이 장애가 발생하면 다른 서버 또는 노드가 대신 처리한다.
        * 시스템 확장이 용이하다.
  * 3.1.2 페이지 캐시
    * 페이지캐시 : 리눅스는 파일 I/O의 성능향상을 위해 페이지 캐시라는 메모리 영역을 만들어서 사용한다. 한번 읽은 파일의 내용을 페이지 캐시라는 영역에 저장시켜 놨다가 다시 한번 동일한 파일 접근이 일어나면 디스크에서 읽지 않고 페이지 캐시에서 읽어서 제공해주는 방식
    * 카프카는 OS의 페이지 캐시를 이용하도록 디자인 됨.
    * 페이지캐시 공유를 이유로, 하나의 시스템에 다른 애플리케이션과 함께 실행하는 것은 권장하지 않음.
  * 3.1.3 배치 전송 처리

* 3.2 카프카 데이터 모델
    * 3.2.1 토픽의 이해
        * 토픽 : 데이터 저장 단위 // 메일 주소와 유사한 개념
        * 컨슈머쪽에서 needs에 맞는 토픽들을 연결하여 메시지 소비 가능
        * 토픽 이름엔 접두어등 규칙을 두어 사용하는 것을 권장
    * 3.2.2 파티션의 이해
        * 프로듀서에서 메시지를 빠르게 토픽에 전송할 수 있도록 파티션과 프로듀서를 늘려주는 것이 좋다.
            * ex) 하나의 프로듀서 & 한개의 파티션 & 4개의 메시지 전송 VS 4개의 프로듀서 & 4개의 파티션 & 4개의 메시지 전송 (병렬 처리 방식)
        * 무조건 파티션 수를 늘리는 것이 좋은가?
            * 파일 핸들러의 낭비 
                * 파티션은 broker의 dir과 매핑
                * 저장되는 데이터 마다 2개의 파일 (인덱스/실제데이터) 
                * 카프카는 모든 디렉토리 파일들에 대해 파일 핸들을 열게되는데 파티션의 수가 많아지면 리소스를 낭비함
            * 장애 복구 시간 증가
                * 높은 가용성을 위해 replication 지원
                    * 브로커에는 토픽이 있고, 토픽에는 여러개의 파티션으로 구성되었을 경우,
                    * 각 파티션마다 replication이 동작하여, 파티션 리더와 팔로워로 구분된다.
                * 이 상태에서 브로커가 다운된다면,
                    * 해당 브로커가 리더일 경우, 일시적으로 사용 불가 상태가 되고,
                    * 팔로워중 하나를 리더로 선출하는 시간이 장애시간으로 이어지게 된다.
                * 여기에 브로커가 컨트롤러인 경우, 
                    * 자동으로 페일오버가 동작하더라도 새컨트롤러 초기화기간 동안 주키퍼에서 모든 파티션의 데이터를 읽어와야해서 장애 기간이 더 길어질 수 있다.
        * 내 토픽의 적절한 파티션의 수는?
            * 원하는 목표 처리량의 기준 잡기
                * ex) 4 프로듀서 , 각 10 msg / sec 처리 가능 & 8 컨슈머,  각 5 msg / sec  
                * 프로듀서 기준으론 4개의 파티션이면 가능하지만, 컨슈머의 기준까지 고려한다면 8개의 파티션이 맞다.
            * 파티션 증가는 가능하지만, 줄이는 방법은 제공하지 않으므로, 점진적으로 수를 늘려가는 방법을 권장
            * 브로커당 최대 2000개의 파티션 수를 권장함
    * 3.2.3 오프셋과 메시지 순서
        * 오프셋: 각 파티션마다 메시지가 저장되는 위치
        * 하나의 파티션 내에서 유일하고, 순차적으로 증가하는 숫자 (64비트 정수)
        * 컨슈머는 오프셋 순서대로만 데이터를 가져갈 수 있음

* 3.3 카프카의 고가용성과 리플리케이션
    * 카프카의 리플리케이션 기준은 토픽이 아닌, 파티션!!
    * 3.3.1 리플리케이션 팩터와 리더, 팔로워의 역할
        * 팩터 : 토픽에 대해 리플리케이션의 수를 지정하는 수
            * ex ) default.replication.factor = 2 // 디폴트값은 1이며, 2로 수정시 토픽의 리플리케이션이 2개로 지정됨
            * 클러스터내 모든 브로커에 동일하게 설정필요. 재시작해야 변경사항 적용됨.
        * 리더 / 팔로워 : rabbitmq의 master queue / mirrored queue 관계와 동일 // 원본과 복제본 개념
            * 주키퍼도 동일하게 리더와 팔로워라고 지칭
            * 리더 : 읽기와 쓰기는 리더에서만 일어남 // 팔로워는 읽기와 쓰기에 관여하지 X
        * 리플리케이션
            * 장점: 장애 리스크를 줄여준다.
            * 단점1: 리플리케이션 수만큼 중복 데이터를 저장하는 등의 낭비
            * 단점2: 브로커의 관리 리소스 사용량 증가 (토픽 상태 체크 및 공기화 리소스)
            * 토픽의 중요도에 따라 팩토를 2, 3으로 나누는 등 구분하여 운영하는 것을 권장
    * 3.3.2 리더와 팔로워의 관리  
        * 팔로워는 주기적으로 리더의 데이터를 기준으로 동기화를 진행
        * 리더가 다운되는 경우, 팔로워 중 하나가 새로운 리더로 승격
        * ISR (In Sync Replica)
            * 현재 리플리케이션 되고 있는 리플리케이션 그룹
            * ISR에 속한 구성원만이 리더의 자격을 가짐
            * 리더는 팔로워들이 주기적으로 데이터를 확인하는지 검토
                * 일정주기 (replica.lag.time.max.ms)를 만족하지 못하면, ISR에서 팔로워 추방
                * ISR 그룹 축소 정보도 팔로워가 리더로부터 정보를 Pull해서 저장

* 3.4 모든 브로커가 다운된다면?
    * 하나씩 브로커가 다운되면서, 카프카 클러스터가 다운되는 상황
    * 해결 방법
        * 1) 마지막 리더가 살아나기를 기다림
            * 메시지 손실없이 작업이 가능
            * 마자미가 리더가 정상화될 때까지 카프카 클러스터의 장애 지속
        * 2) ISR에서 추방되었지만 먼저 살아나면 자동으로 리더
            * 메시지 손실이 발생할 수 있음
            * 마지막 리더가 아닌 팔로워가 새로운 리더가 되었을 경우, 마지막 리더가 뒤늦게 정상화되더라도 새로운 리더 데이터 동기화를 하게 됨 (메시지 손실 발생)
            * 하지만, 빠르게 정상화 가능
    * unclean.leader.election.enable 옵션으로 위 두가지 해결 방법을 고를 수 있다.
        * true : 2) 해결법 / false : 1) 해결법

* 3.5 카프카에서 사용하는 주키퍼 지노드 역할 // 이쪽은 그냥 한번 읽어보세요~
    * controller : 클러스터의 컨트롤러 정보
        *  브로커 레벨에서 컨트롤러를 선정
        *  실패 감지시, 실패한 브로커에 의해 영향받는 모든 파티션의 리더를 변경
        *  클러스터내 브로커가 임의로 선정되고 다운시, 남아있는 브로커 중 새로운 컨트롤러가 지정됨.
    * brokers : 브로커 관련 정보
        * ids의 경우, 임시노드로 저장
    * consumers : 컨슈머 관련 정보
        * offset의 경우, 지워지지 않아야할 중요정보이므로 영구 노드로 저장됨 
    * config : 토픽의 상세 설정 정보 확인 가능.